{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://docs.databricks.com/_static/notebooks/structured-streaming-kafka.html\n",
    "\n",
    "This is a WordCount example with the following\n",
    "\n",
    "Kafka as a Structured Streaming Source\n",
    "Stateful operation (groupBy) to calculate running counts\n",
    "Requirements\n",
    "An Apache Kafka 0.10.0 (or above) deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{explode, split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@16d74392\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@16d74392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"SparkByExample\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First SparkContext:\n",
      "APP Name :SparkByExample\n",
      "Deploy Mode :client\n",
      "Master :spark://master:7077\n"
     ]
    }
   ],
   "source": [
    "println(\"First SparkContext:\")\n",
    "println(\"APP Name :\"+spark.sparkContext.appName);\n",
    "println(\"Deploy Mode :\"+spark.sparkContext.deployMode);\n",
    "println(\"Master :\"+spark.sparkContext.master);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:42: error: overloaded method value option with alternatives:\n  (key: String,value: Double)org.apache.spark.sql.streaming.DataStreamReader <and>\n  (key: String,value: Long)org.apache.spark.sql.streaming.DataStreamReader <and>\n  (key: String,value: Boolean)org.apache.spark.sql.streaming.DataStreamReader <and>\n  (key: String,value: String)org.apache.spark.sql.streaming.DataStreamReader\n cannot be applied to (String, String, String)\n       option(\"subscribe\", \"sample_topic\", \"android\"). // comma separated list of topics\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// Subscribe to 1 topic\n",
    "val df = spark.readStream.\n",
    "format(\"kafka\").\n",
    "option(\"kafka.bootstrap.servers\", \"kafka:9092\"). // comma separated list of broker:host\n",
    "option(\"subscribe\", \"sample_topic\", \"android\"). // comma separated list of topics\n",
    "//option(\"startingOffsets\", \"latest\"). // read data from the end of the stream\n",
    "option(\"startingOffsets\", \"earliest\").\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:37: error: not found: value df\n       println(df)\n               ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "println(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:37: error: not found: value kafka\n       val df = kafka.select(explode(split($\"value\".cast(\"string\"), \"\\\\s+\")).as(\"word\")).\n                ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// split lines by whitespace and explode the array as rows of `word`\n",
    "val df = kafka.select(explode(split($\"value\".cast(\"string\"), \"\\\\s+\")).as(\"word\")).\n",
    "groupBy($\"word\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Unknown Error",
     "evalue": "<console>:38: error: not found: value display\n       display(df.select($\"word\", $\"count\"))\n       ^\n<console>:38: error: not found: value df\n       display(df.select($\"word\", $\"count\"))\n               ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "// follow the word counts as it updates\n",
    "display(df.select($\"word\", $\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try it yourself\n",
    "The Kafka Source also includes the ingestion timestamp of records. Try counting the words by the ingestion time window as well.\n",
    "\n",
    "Note: Kafka provides timestamps in milliseconds. In order to be able to cast the long value to timestamp, you will have to divide the timestamp by 1000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
