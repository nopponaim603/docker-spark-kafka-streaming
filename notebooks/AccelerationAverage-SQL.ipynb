{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceleration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:26: error: object kafka is not a member of package org.apache\n       import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n                         ^\n<console>:33: error: object kafka is not a member of package org.apache\n       import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n                         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, Minutes, StreamingContext}\n",
    "import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import collection.JavaConverters.mapAsJavaMapConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:27: error: not found: type StreamingContext\n       val ssc = new StreamingContext(sc, Seconds(1))\n                     ^\n<console>:27: error: not found: value Seconds\n       val ssc = new StreamingContext(sc, Seconds(1))\n                                          ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(1))\n",
    "ssc.remember(Minutes(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kafka input stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:27: error: not found: type BytesDeserializer\n         \"key.deserializer\" -> classOf[BytesDeserializer],\n                                       ^\n<console>:28: error: not found: type StringDeserializer\n         \"value.deserializer\" -> classOf[StringDeserializer],\n                                         ^\n<console>:35: error: not found: value KafkaUtils\n       val stream = KafkaUtils.createDirectStream[String, String](\n                    ^\n<console>:36: error: not found: value ssc\n         ssc,\n         ^\n<console>:37: error: not found: value PreferConsistent\n         PreferConsistent,\n         ^\n<console>:38: error: not found: value Subscribe\n         Subscribe[String, String](topics, consumerParams)\n         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val consumerParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"kafka:9092\",\n",
    "  \"key.deserializer\" -> classOf[BytesDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"spark-notebook\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "val topics = Array(\"sample_topic\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, consumerParams)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Input Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:25: error: not found: type StructType\n       val schema = new StructType().add(\"time\", \"long\").add(\"x\", \"float\").add(\"y\", \"float\").add(\"z\", \"float\")\n                        ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "val schema = new StructType().add(\"time\", \"long\").add(\"x\", \"float\").add(\"y\", \"float\").add(\"z\", \"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:28: error: not found: value stream\n       stream.foreachRDD { rdd =>\n       ^\n<console>:29: error: package schema is not a value\n         spark.read.schema(schema).json(rdd.map(_.value())).createOrReplaceTempView(\"locations\")\n                           ^\n<console>:34: error: not found: value ProducerConfig\n               ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> \"kafka:9092\",\n               ^\n<console>:35: error: not found: value ProducerConfig\n               ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\",\n               ^\n<console>:36: error: not found: value ProducerConfig\n               ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\"\n               ^\n<console>:39: error: not found: type KafkaProducer\n             val producer = new KafkaProducer[String, String](producerParams.asJava)\n                                ^\n<console>:39: error: value asJava is not a member of scala.collection.immutable.Map[String,Object]\n             val producer = new KafkaProducer[String, String](producerParams.asJava)\n                                                                             ^\n<console>:41: error: value foreach is not a member of Object\n             partition.foreach { s =>\n                       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "stream.foreachRDD { rdd =>\n",
    "  spark.read.schema(schema).json(rdd.map(_.value())).createOrReplaceTempView(\"locations\")\n",
    "  spark.sql(\"select avg(x) as x, avg(y) as y, avg(z) as z, min(timestamp) as timestamp from locations\").toJSON.foreachPartition {\n",
    "    partition =>\n",
    "\n",
    "      val producerParams = Map[String, Object](\n",
    "        ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> \"kafka:9092\",\n",
    "        ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\",\n",
    "        ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\"\n",
    "      )\n",
    " \n",
    "      val producer = new KafkaProducer[String, String](producerParams.asJava)\n",
    "      \n",
    "      partition.foreach { s =>\n",
    "        if (s != \"{}\")\n",
    "            println(s)\n",
    "            producer.send(new ProducerRecord[String, String](\"sample_topic\", s))\n",
    "            //acceleration\n",
    "      }\n",
    "      \n",
    "      producer.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Compile Error",
     "evalue": "<console>:26: error: not found: value ssc\n       ssc.start()\n       ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see what we really read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Magic SQL failed to execute with error: \n",
       "Table or view not found: locations; line 1 pos 14;\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [locations], [], false\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%SQL\n",
    "select * from locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"streaming-job-executor-0\" java.lang.Error: java.lang.InterruptedException\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1148)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "Caused by: java.lang.InterruptedException\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:623)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:902)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:900)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n",
      "\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n",
      "\tat org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2126)\n",
      "\tat $line35.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:45)\n",
      "\tat $line35.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:43)\n",
      "\tat org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n",
      "\tat org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\t... 2 more\n"
     ]
    }
   ],
   "source": [
    "StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Verify the contents in Kafka using the console consumer\n",
    "\n",
    "The following command line tools can help print the contents to the console.\n",
    "```sh\n",
    "./bin/kafka-console-consumer.sh --topic acceleration --bootstrap-server localhost:9092\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
