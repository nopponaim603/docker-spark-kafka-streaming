{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Acceleration Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Unknown Error",
     "evalue": "<console>:43: error: object kafka is not a member of package org.apache\n       import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n                         ^\n<console>:44: error: object kafka is not a member of package org.apache\n       import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n                         ^\n<console>:45: error: object kafka is not a member of package org.apache\n       import org.apache.kafka.sql\n                         ^\n",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "import org.apache.spark.streaming.{Seconds, Minutes, StreamingContext}\n",
    "import org.apache.spark.streaming.kafka010.KafkaUtils\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.StructType\n",
    "\n",
    "import org.apache.kafka.common.serialization.{BytesDeserializer, StringDeserializer}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.kafka.sql\n",
    "\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n",
    "\n",
    "import collection.JavaConverters.mapAsJavaMapConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark_jars = org.apache.hadoop:hadoop-aws:3.2.0,org.postgresql:postgresql:42.2.18,org.apache.spark:spark-avro_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.6.0,com.databricks:spark-xml_2.12:0.12.0\n",
       "spark = org.apache.spark.sql.SparkSession@412c9ec\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@412c9ec"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "val spark_jars = \"org.apache.hadoop:hadoop-aws:3.2.0,org.postgresql:postgresql:42.2.18,org.apache.spark:spark-avro_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.kafka:kafka-clients:2.6.0,com.databricks:spark-xml_2.12:0.12.0\"\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").config(\"spark.jars.packages\", spark_jars).getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "java.lang.NoClassDefFoundError",
     "evalue": "Could not initialize class org.apache.spark.sql.kafka010.KafkaSourceProvider$",
     "output_type": "error",
     "traceback": [
      "java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.sql.kafka010.KafkaSourceProvider$",
      "  at org.apache.spark.sql.kafka010.KafkaSourceProvider.org$apache$spark$sql$kafka010$KafkaSourceProvider$$validateStreamOptions(KafkaSourceProvider.scala:326)",
      "  at org.apache.spark.sql.kafka010.KafkaSourceProvider.sourceSchema(KafkaSourceProvider.scala:71)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:236)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:117)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:117)",
      "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:33)",
      "  at org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:219)",
      "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:194)",
      "  ... 48 elided"
     ]
    }
   ],
   "source": [
    "// Subscribe to 1 topic\n",
    "// read from Kafka \n",
    "// org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3\n",
    "val inputDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:9092\").option(\"subscribe\", \"sample_topic\").load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(1))\n",
    "ssc.remember(Minutes(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup Kafka input stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val consumerParams = Map[String, Object](\n",
    "  \"bootstrap.servers\" -> \"localhost:9092\",\n",
    "  \"key.deserializer\" -> classOf[BytesDeserializer],\n",
    "  \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "  \"group.id\" -> \"spark-notebook\",\n",
    "  \"auto.offset.reset\" -> \"earliest\",\n",
    "  \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    ")\n",
    "\n",
    "val topics = Array(\"sample-topic\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, consumerParams)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Expected Input Schema\n",
    "\n",
    "{\"time\":1990,\"type\":\"DOTA_COMBATLOG_DAMAGE\",\"value\":4,\"attackername\":\"npc_dota_hero_huskar\",\"targetname\":\"npc_dota_hero_huskar\",\"sourcename\":\"npc_dota_hero_huskar\",\"targetsourcename\":\"npc_dota_hero_huskar\",\"attackerhero\":true,\"targethero\":true,\"attackerillusion\":false,\"targetillusion\":false,\"inflictor\":\"item_armlet\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val schema = new StructType().add(\"time\", \"long\").add(\"type\", \"string\").add(\"value\", \"long\").add(\"attackername\", \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stream Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stream.foreachRDD { rdd =>\n",
    "    spark.read.schema(schema).json(rdd.map(_.value())).createOrReplaceTempView(\"logs\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stream.foreachRDD { rdd =>\n",
    "  spark.read.schema(schema).json(rdd.map(_.value())).createOrReplaceTempView(\"logs\")\n",
    "                   \n",
    "  spark.sql(\"select * FROM logs\").toJSON.foreachPartition {\n",
    "    partition =>\n",
    "\n",
    "      val producerParams = Map[String, Object](\n",
    "        ProducerConfig.BOOTSTRAP_SERVERS_CONFIG -> \"localhost:9092\",\n",
    "        ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\",\n",
    "        ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringSerializer\"\n",
    "      )\n",
    " \n",
    "      val producer = new KafkaProducer[String, String](producerParams.asJava)\n",
    "      \n",
    "      partition.foreach { s =>\n",
    "        if (s != \"{}\")\n",
    "            producer.send(new ProducerRecord[String, String](\"sample-topic\", s))\n",
    "      }\n",
    "      \n",
    "      producer.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Start stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lets see what we really read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%SQL\n",
    "select * from logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stop stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Verify the contents in Kafka using the console consumer\n",
    "\n",
    "The following command line tools can help print the contents to the console.\n",
    "```sh\n",
    "./bin/kafka-console-consumer.sh --topic sample-topic --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// Subscribe to 1 topic\n",
    "// read from Kafka \n",
    "// org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\n",
    "val inputDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"sample-topic\").load()\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Stop stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Verify the contents in Kafka using the console consumer\n",
    "\n",
    "The following command line tools can help print the contents to the console.\n",
    "```sh\n",
    "./bin/kafka-console-consumer.sh --topic sample-topic --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.ClassNotFoundException\n",
       "Message: Failed to find data source: kafka. Please find packages at https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:148)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:79)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:79)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:218)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:80)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:80)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n",
       "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:124)\n",
       "  ... 50 elided\n",
       "Caused by: java.lang.ClassNotFoundException: kafka.DefaultSource\n",
       "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\n",
       "  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
       "  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n",
       "  at scala.util.Try$.apply(Try.scala:192)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n",
       "  at scala.util.Try.orElse(Try.scala:84)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:132)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Subscribe to 1 topic\n",
    "// read from Kafka \n",
    "// org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\n",
    "val inputDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"sample-topic\").load()\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
