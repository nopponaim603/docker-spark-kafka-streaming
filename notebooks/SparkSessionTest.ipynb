{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://docs.databricks.com/_static/notebooks/structured-streaming-kafka.html\n",
    "\n",
    "This is a WordCount example with the following\n",
    "\n",
    "Kafka as a Structured Streaming Source\n",
    "Stateful operation (groupBy) to calculate running counts\n",
    "Requirements\n",
    "An Apache Kafka 0.10.0 (or above) deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{explode, split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val spark = SparkSession.builder().appName(\"SparkByExample\").getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First SparkContext:\n",
      "APP Name :Apache Toree\n",
      "Deploy Mode :client\n",
      "Master :spark://master:7077\n"
     ]
    }
   ],
   "source": [
    "println(\"First SparkContext:\")\n",
    "println(\"APP Name :\"+spark.sparkContext.appName);\n",
    "println(\"Deploy Mode :\"+spark.sparkContext.deployMode);\n",
    "println(\"Master :\"+spark.sparkContext.master);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.ClassNotFoundException\n",
       "Message: Failed to find data source: kafka. Please find packages at https://cwiki.apache.org/confluence/display/SPARK/Third+Party+Projects\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:148)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:79)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:79)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:218)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:80)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:80)\n",
       "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n",
       "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:124)\n",
       "  ... 42 elided\n",
       "Caused by: java.lang.ClassNotFoundException: kafka.DefaultSource\n",
       "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)\n",
       "  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
       "  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5$$anonfun$apply$1.apply(DataSource.scala:132)\n",
       "  at scala.util.Try$.apply(Try.scala:192)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$5.apply(DataSource.scala:132)\n",
       "  at scala.util.Try.orElse(Try.scala:84)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:132)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Subscribe to 1 topic\n",
    "val df = spark.readStream.\n",
    "format(\"kafka\").\n",
    "option(\"kafka.bootstrap.servers\", \"localhost:9092,10.96.0.20:9096\"). // comma separated list of broker:host\n",
    "option(\"subscribe\", \"sample_topic,android\"). // comma separated list of topics\n",
    "option(\"startingOffsets\", \"latest\"). // read data from the end of the stream\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// split lines by whitespace and explode the array as rows of `word`\n",
    "val df = kafka.select(explode(split($\"value\".cast(\"string\"), \"\\\\s+\")).as(\"word\")).\n",
    "groupBy($\"word\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:19: error: not found: value df\n",
       "       df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n",
       "       ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// follow the word counts as it updates\n",
    "display(df.select($\"word\", $\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try it yourself\n",
    "The Kafka Source also includes the ingestion timestamp of records. Try counting the words by the ingestion time window as well.\n",
    "\n",
    "Note: Kafka provides timestamps in milliseconds. In order to be able to cast the long value to timestamp, you will have to divide the timestamp by 1000."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}